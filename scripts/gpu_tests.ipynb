{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices (CPUs and GPUs)\n",
    "physical_devices = tf.config.experimental.list_physical_devices()\n",
    "\n",
    "# List only GPUs\n",
    "gpus = [device for device in physical_devices if device.device_type == 'GPU']\n",
    "\n",
    "if gpus:\n",
    "    # Print GPU information\n",
    "    for gpu in gpus:\n",
    "        print(\"GPU Name:\", gpu.name)\n",
    "else:\n",
    "    print(\"No GPUs found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.MCTS as MCTS\n",
    "from src.Environments import StatelessGym\n",
    "from src.Experiment import Experiment, RandomExperiment, ParametrizedRandomExperiment\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_log_error, max_error\n",
    "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, mean_tweedie_deviance\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import ast\n",
    "import math\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "def encode_maze(maze):\n",
    "    num_rows = len(maze)\n",
    "    num_cols = len(maze[0])\n",
    "\n",
    "    encoded_maze = []\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            if maze[i][j] == 'S':\n",
    "                encoded_maze.append(0)\n",
    "            elif maze[i][j] == 'F':\n",
    "                encoded_maze.append(1)\n",
    "            elif maze[i][j] == 'H':\n",
    "                encoded_maze.append(2)\n",
    "            elif maze[i][j] == 'G':\n",
    "                encoded_maze.append(3)\n",
    "            elif maze[i][j] == 'E':\n",
    "                encoded_maze.append(4)\n",
    "    return encoded_maze\n",
    "\n",
    "def add_padding(map, target_size):\n",
    "    current_size = len(map)\n",
    "    diff = target_size - current_size\n",
    "    if diff < 0:\n",
    "        raise Exception(\"Current map size is greater than target size\")\n",
    "    elif diff == 0:\n",
    "        return map\n",
    "    else:\n",
    "        result = []\n",
    "        padding = diff // 2\n",
    "        left_out = diff % 2\n",
    "        for i in range(padding):\n",
    "            result.append('E' * target_size)\n",
    "        for row in map:\n",
    "            new_row = 'E' * padding + row + 'E' * padding + 'E' * left_out\n",
    "            result.append(new_row)\n",
    "        for i in range(padding + left_out):\n",
    "            result.append('E' * target_size)\n",
    "        return result\n",
    "    \n",
    "def encode_map(map, categories='auto'):\n",
    "    # Convert the map to a 2D array\n",
    "    map_array =  []\n",
    "    for row in map:\n",
    "        for letter in row:\n",
    "            map_array.append([letter])\n",
    "    # Create an instance of OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse=False, categories=categories)\n",
    "\n",
    "    # Fit and transform the map array\n",
    "    encoded_map = encoder.fit_transform(map_array).astype('int64')\n",
    "\n",
    "    # Get the categories (unique values) from the encoder\n",
    "    categories = encoder.categories_[0]\n",
    "\n",
    "    # Create a dictionary to map the encoded values to the original categories\n",
    "    category_mapping = {i: category for i, category in enumerate(categories)}\n",
    "\n",
    "    # Return the encoded map and the category mapping\n",
    "    return encoded_map, encoder.categories_, category_mapping\n",
    "\n",
    "            \n",
    "def file_dir(relative_path):\n",
    "    absolute_path = os.path.dirname(__file__)\n",
    "    return os.path.join(absolute_path, relative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../datasets/FrozenLake-v1_m4-4_s1-100_t1_unboosted/\"\n",
    "dataset_names = os.listdir(directory)\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset = dataset.append(pd.read_csv(directory + dataset_name), ignore_index=True)\n",
    "    \n",
    "padding = 4\n",
    "\n",
    "one_hot = False\n",
    "\n",
    "if 'Map' in dataset.columns:\n",
    "    if padding > 0: \n",
    "        dataset['List_Map'] = dataset['Map'].apply(ast.literal_eval).apply(lambda x: add_padding(x, padding))\n",
    "    else: \n",
    "        dataset['List_Map'] = dataset['Map'].apply(ast.literal_eval)\n",
    "    #dataset['F_count'] = dataset['Map'].apply(lambda x: sum(row.count('F') for row in x))\n",
    "    dataset['Encoded_Map'] = dataset['List_Map'].apply(lambda x: encode_maze(x))\n",
    "    if one_hot:\n",
    "        categories = encode_map(dataset['List_Map'].iloc[0])[1]\n",
    "        dataset['OneHotEncoded_Map'] = dataset['List_Map'].apply(lambda x: np.reshape(encode_map(x, categories)[0], (-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "#'LinearRegression': LinearRegression(),\n",
    "# #'Ridge': Ridge(alpha=1.0),\n",
    "# #'Lasso': Lasso(alpha=1.0),\n",
    "# #'ElasticNet':  ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "#'SVR': SVR(),\n",
    "#'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "#'RandomForestRegressor': RandomForestRegressor(),\n",
    "'GradientBoostingRegressor': GradientBoostingRegressor(n_estimators=200, max_depth=20),\n",
    "#'KNeighborsRegressor': KNeighborsRegressor(n_neighbors=5),\n",
    "#'MLPRegressor': MLPRegressor(hidden_layer_sizes=(200, 200, 200, 200, 200), activation='tanh', max_iter=1000000, n_iter_no_change=10, tol=1e-4)\n",
    "}\n",
    "\n",
    "#Getting min and max number of simulations\n",
    "sim_min = dataset['Simulations'].min()\n",
    "sim_max = dataset['Simulations'].max()\n",
    "\n",
    "#Features to be used in the model\n",
    "features = ['Simulations']\n",
    "\n",
    "#Unique maps\n",
    "unique_maps = []\n",
    "for map in dataset[\"Map\"].unique():\n",
    "    unique_maps.append(map)\n",
    "map_count = len(unique_maps)\n",
    "\n",
    "#Folds\n",
    "fold = 1\n",
    "\n",
    "#Test set size\n",
    "test_set_size = math.ceil((map_count * 0.33))\n",
    "#test_set_size = len(unique_maps) - 50\n",
    "#Train set sizes\n",
    "#train_sizes = [1, 8, 16, 25, 75, 100, 1000, 2000, 3000, 4000, 5000, 10000, 15000, 30000, 60000, 100000, 150000, 200000, 250000, 300000]\n",
    "#train_sizes = train_sizes = list(range(10, 1000, 125)) + list(range(1000, 10000, 1000))\n",
    "#train_sizes = [20000]\n",
    "train_sizes = [320000]\n",
    "#train_sizes = [1, 25, 100]\n",
    "\n",
    "train_scores1 = []\n",
    "train_scores2 = []\n",
    "test_scores = []\n",
    "\n",
    "categories = encode_map(dataset['List_Map'].iloc[0])[1]\n",
    "test_maps = np.random.default_rng().choice(unique_maps, size=test_set_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x1a2b8d60848>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.device('/GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "10000/10000 [==============================] - 19s 2ms/step - loss: 0.0895\n",
      "Epoch 2/1000\n",
      "10000/10000 [==============================] - 18s 2ms/step - loss: 0.0719\n",
      "Epoch 3/1000\n",
      "10000/10000 [==============================] - 17s 2ms/step - loss: 0.0679\n",
      "Epoch 4/1000\n",
      "10000/10000 [==============================] - 18s 2ms/step - loss: 0.0689\n",
      "Epoch 5/1000\n",
      "10000/10000 [==============================] - 20s 2ms/step - loss: 0.0667\n",
      "Epoch 6/1000\n",
      "10000/10000 [==============================] - 20s 2ms/step - loss: 0.0652\n",
      "Epoch 7/1000\n",
      "10000/10000 [==============================] - 90s 9ms/step - loss: 0.0655\n",
      "Epoch 8/1000\n",
      "10000/10000 [==============================] - 103s 10ms/step - loss: 0.0646\n",
      "Epoch 9/1000\n",
      "10000/10000 [==============================] - 102s 10ms/step - loss: 0.0646\n",
      "Epoch 10/1000\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 0.0640\n",
      "Epoch 11/1000\n",
      " 1802/10000 [====>.........................] - ETA: 13s - loss: 0.0639"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26320\\3747509436.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m         ])\n\u001b[0;32m     69\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_set_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;31m#Predicting on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\onerk\\anaconda3\\envs\\MCTS-Time-Management\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\onerk\\anaconda3\\envs\\MCTS-Time-Management\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2496\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2497\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2499\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1863\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_string = \"\"\n",
    "for training_set_size in  train_sizes:\n",
    "    train_scores1.append([])\n",
    "    train_scores2.append([])\n",
    "    test_scores.append([])\n",
    "    \n",
    "    for i in range(fold):\n",
    "        #Creating Test Set\n",
    "        \n",
    "        test_set = dataset[dataset['Map'].isin(test_maps)].groupby([\"Map\", \"Simulations\"]).mean()[\"Discounted Return\"]\n",
    "        test_set_x = []\n",
    "        test_set_y = []\n",
    "        for j in range(len(test_set)):\n",
    "            if one_hot:\n",
    "                test_set_x.append([test_set.index[j][1]] + list(np.reshape(encode_map(add_padding(ast.literal_eval(test_set.index[j][0]), padding), categories=categories)[0], (-1))))\n",
    "            else:\n",
    "                test_set_x.append([test_set.index[j][1]] + encode_maze(add_padding(ast.literal_eval(test_set.index[j][0]), padding)))\n",
    "            test_set_y.append(test_set[j])\n",
    "\n",
    "        #Creating Training Set\n",
    "        training_set = dataset[~dataset['Map'].isin(test_maps)]\n",
    "        training_set_sampled = training_set.sample(n=training_set_size, replace=True)\n",
    "        if one_hot:\n",
    "            training_set_x = np.append(training_set_sampled[\"Simulations\"].values.reshape(-1, 1), training_set_sampled['OneHotEncoded_Map'].apply(pd.Series).values, axis=1).astype('int64')\n",
    "        else:\n",
    "            training_set_x = np.append(training_set_sampled[\"Simulations\"].values.reshape(-1, 1), training_set_sampled['Encoded_Map'].apply(pd.Series).values, axis=1)\n",
    "        training_set_y = training_set_sampled[\"Discounted Return\"].values\n",
    "        \n",
    "        #print(\"Maps seen in training: %d\" % (len(training_set[\"Map\"].unique())))\n",
    "        #Creating Training Score 1 Set - Looking only sampled points\n",
    "        training_score1_set = training_set_sampled.groupby([\"Map\", \"Simulations\"]).mean()[\"Discounted Return\"]\n",
    "        training_score1_set_x = []\n",
    "        training_score1_set_y = []\n",
    "        for j in range(len(training_score1_set)):\n",
    "            if one_hot:\n",
    "                training_score1_set_x.append([training_score1_set.index[j][1]] + list(np.reshape(encode_map(add_padding(ast.literal_eval(training_score1_set.index[j][0]), padding), categories=categories)[0], (-1))))\n",
    "            else:\n",
    "                training_score1_set_x.append([training_score1_set.index[j][1]] + encode_maze(add_padding(ast.literal_eval(training_score1_set.index[j][0]), padding)))\n",
    "            training_score1_set_y.append(training_score1_set[j])\n",
    "            \n",
    "        #Creating Training Score 2 Set - Looking all datapoints in the training sample\n",
    "        training_sampled_unique = training_set_sampled[\"Map\"].unique()\n",
    "        training_score2_set = training_set[training_set[\"Map\"].isin(training_sampled_unique)].groupby([\"Map\", \"Simulations\"]).mean()[\"Discounted Return\"]\n",
    "        training_score2_set_x = []\n",
    "        training_score2_set_y = []\n",
    "        for i in range(len(training_score2_set)):\n",
    "            if one_hot:\n",
    "                training_score2_set_x.append([training_score2_set.index[j][1]] + list(np.reshape(encode_map(add_padding(ast.literal_eval(training_score2_set.index[j][0]), padding), categories=categories)[0], (-1))))\n",
    "            else:\n",
    "                training_score2_set_x.append([training_score2_set.index[i][1]] + encode_maze(training_score2_set.index[i][0]))\n",
    "            training_score2_set_y.append(training_score2_set[i])\n",
    "\n",
    "\n",
    "        # print(\"train score set info: \")\n",
    "        # a = (training_set.groupby([\"Map\", \"Simulations\"]).std()[\"Discounted Return\"] / (training_set.groupby([\"Map\", \"Simulations\"]).count()[\"Discounted Return\"] ** 0.5))\n",
    "        # print(a.mean())\n",
    "        # print(\"test set info: \")\n",
    "        # b = (dataset[dataset['Map'].isin(test_maps)].groupby([\"Map\", \"Simulations\"]).std()[\"Discounted Return\"] / (dataset[dataset['Map'].isin(test_maps)].groupby([\"Map\", \"Simulations\"]).count()[\"Discounted Return\"] ** 0.5))\n",
    "        # print(b.mean())\n",
    "        #Training\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=200, activation='tanh', input_shape=(17,)),\n",
    "            tf.keras.layers.Dense(units=200, activation='tanh'),\n",
    "            tf.keras.layers.Dense(units=200, activation='tanh'),\n",
    "            tf.keras.layers.Dense(units=200, activation='tanh'),\n",
    "            tf.keras.layers.Dense(units=200, activation='tanh'),\n",
    "            tf.keras.layers.Dense(units=1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model.fit(np.asarray(training_set_x).astype('float32'), training_set_y, epochs=1000, verbose = 1)\n",
    "        #Predicting on test set\n",
    "        y_pred = model.predict(np.asarray(test_set_x).astype('float32'))\n",
    "        test_score = mean_squared_error(test_set_y, y_pred)\n",
    "        test_scores[-1].append(test_score)\n",
    "        \n",
    "        #Predicting on training score 1 set\n",
    "        y_pred = model.predict(np.asarray(training_score1_set_x).astype('float32'))\n",
    "        train_score1 = mean_squared_error(training_score1_set_y, y_pred)\n",
    "        train_scores1[-1].append(train_score1)\n",
    "        \n",
    "        #Predicting on training score 2 set\n",
    "        y_pred = model.predict(np.asarray(training_score2_set_x).astype('float32'))\n",
    "        train_score2 = mean_squared_error(training_score2_set_y, y_pred)\n",
    "        train_scores2[-1].append(train_score2)\n",
    "            \n",
    "        # print(\"Fold: %d\\nTraining set size: %d\\nTraining error: %f\\nTest error: %f\\n\" % (i, training_set_size, train_score, test_score))\n",
    "    print(\"Training set size: %d\\nTraining error 1: %f ± %f\\nTraining error 2: %f ± %f\\nTest error: %f ± %f\\n\" % (training_set_size, np.mean(train_scores1[-1]), np.std(train_scores1[-1]) / (fold ** 0.5), np.mean(train_scores2[-1]), np.std(train_scores2[-1]) / (fold ** 0.5), np.mean(test_scores[-1]), np.std(test_scores[-1]) / (fold ** 0.5)))\n",
    "\n",
    "# Calculate the mean and standard deviation of the training and test scores\n",
    "train1_mean = np.mean(train_scores1, axis=1)\n",
    "train1_std = np.std(train_scores1, axis=1) / (fold ** 0.5)\n",
    "train2_mean = np.mean(train_scores2, axis=1)\n",
    "train2_std = np.std(train_scores2, axis=1) / (fold ** 0.5)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1) / (fold ** 0.5)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train1_mean, marker='o', markersize=4, label='Training error 1')\n",
    "plt.plot(train_sizes, train2_mean, marker='o', markersize=4, label='Training error 2')\n",
    "plt.plot(train_sizes, test_mean, marker='o', markersize=4, label='Test error')\n",
    "\n",
    "# Add error bands showing the standard deviation\n",
    "plt.fill_between(train_sizes, train1_mean - train1_std, train1_mean + train1_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, train2_mean - train2_std, train2_mean + train2_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim([0.0, 0.5])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCTS-Time-Management",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
