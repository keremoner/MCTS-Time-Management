JobId=8592316 JobName=analysis.sbatch
   UserId=koner(565562) GroupId=domain users(100513) MCS_label=N/A
   Priority=18692737 Nice=0 Account=ewi-insy-ii-influence QOS=medium
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   DerivedExitCode=0:0
   RunTime=00:00:01 TimeLimit=04:10:00 TimeMin=N/A
   SubmitTime=2023-07-17T14:48:51 EligibleTime=2023-07-17T14:48:51
   AccrueTime=2023-07-17T14:48:51
   StartTime=2023-07-17T14:49:16 EndTime=2023-07-17T18:59:16 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-07-17T14:49:16 Scheduler=Backfill
   Partition=general AllocNode:Sid=login1:27418
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=grs2
   BatchHost=grs2
   NumNodes=1 NumCPUs=10 NumTasks=1 CPUs/Task=10 ReqB:S:C:T=0:0:*:*
   TRES=cpu=10,mem=2G,node=1,billing=5
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   JOB_GRES=(null)
     Nodes=grs2 CPU_IDs=12-15,26-31 Mem=2048 GRES=
   MinCPUsNode=10 MinMemoryNode=2G MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/winhome/koner/Desktop/MCTS-Time-Management/jobscripts/analysis.sbatch
   WorkDir=/winhome/koner/Desktop/MCTS-Time-Management/jobscripts
   StdErr=/winhome/koner/Desktop/MCTS-Time-Management/jobscripts/slurm-8592316.out
   StdIn=/dev/null
   StdOut=/winhome/koner/Desktop/MCTS-Time-Management/jobscripts/slurm-8592316.out
   Power=
   MailUser=koner MailType=END
   

[learning_curve] Training set sizes: [  1208   3382   5556   7731   9905  12080  24160  36240  48320  60400
  72480  84560  96640 108720 120800]
[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.
Traceback (most recent call last):
  File "../scripts/Analysis.py", line 117, in <module>
    train_sizes, train_scores, test_scores = learning_curve(models['MLPRegressor'], X[:size], y[:size], cv=5, train_sizes=np.append(np.linspace(0.01, 0.1, 5, endpoint=False), np.linspace(0.1, 1.0, 10)), scoring='r2', n_jobs=cores, verbose=2)
  File "/home/nfs/koner/.conda/envs/MCTS-Time-Management/lib/python3.7/site-packages/sklearn/model_selection/_validation.py", line 1568, in learning_curve
    for train, test in train_test_proportions
  File "/home/nfs/koner/.conda/envs/MCTS-Time-Management/lib/python3.7/site-packages/joblib/parallel.py", line 1944, in __call__
    return output if self.return_generator else list(output)
  File "/home/nfs/koner/.conda/envs/MCTS-Time-Management/lib/python3.7/site-packages/joblib/parallel.py", line 1587, in _get_outputs
    yield from self._retrieve()
  File "/home/nfs/koner/.conda/envs/MCTS-Time-Management/lib/python3.7/site-packages/joblib/parallel.py", line 1691, in _retrieve
    self._raise_error_fast()
  File "/home/nfs/koner/.conda/envs/MCTS-Time-Management/lib/python3.7/site-packages/joblib/parallel.py", line 1726, in _raise_error_fast
    error_job.get_result(self.timeout)
  File "/home/nfs/koner/.conda/envs/MCTS-Time-Management/lib/python3.7/site-packages/joblib/parallel.py", line 735, in get_result
    return self._return_or_raise()
  File "/home/nfs/koner/.conda/envs/MCTS-Time-Management/lib/python3.7/site-packages/joblib/parallel.py", line 753, in _return_or_raise
    raise self._result
joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.

The exit codes of the workers are {SIGKILL(-9)}
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=8592316.0. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: grs2: task 0: Out Of Memory
srun: launch/slurm: _step_signal: Terminating StepId=8592316.0
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=8592316.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
